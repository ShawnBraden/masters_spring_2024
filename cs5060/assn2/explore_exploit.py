import numpy as np
import random
import matplotlib.pyplot as plt

#simulation pramaters
num_machines = 21
convergance = 1.5
sim_iter = 10000
drift = True

def get_probabilities(drift=0, time=0):
    if time < 300:
        probs = [
            np.random.normal(0-(drift * time), 5),
            np.random.normal(-0.5-(drift * time),12),
            np.random.normal(2-(drift * time),3.9),
            np.random.normal(-0.5-(drift * time),7),
            np.random.normal(-1.2-(drift * time),8),
            np.random.normal(-3-(drift * time),7),
            np.random.normal(-10-(drift * time),20),
            np.random.normal(-0.5-(drift * time),1),
            np.random.normal(-1-(drift * time),2),
            np.random.normal(1-(drift * time),6),
            np.random.normal(0.7-(drift * time),4),
            np.random.normal(-6-(drift * time),11),
            np.random.normal(-7-(drift * time),1),
            np.random.normal(-0.5-(drift * time),2),
            np.random.normal(-6.5-(drift * time),1),
            np.random.normal(-3-(drift * time),6),
            np.random.normal(0-(drift * time),8),
            np.random.normal(2-(drift * time),3.9),
            np.random.normal(-9-(drift * time),12),
            np.random.normal(-1-(drift * time),6),
            np.random.normal(-4.5-(drift * time),8)              
        ]
    else: 
        mean_7 = -0.5-(drift * time + 1)
        x = np.random.normal(mean_7,1) 

        probs = [
            np.random.normal(0-(drift * time) + 7, 5),
            np.random.normal(-0.5-(drift * time),12),
            np.random.normal(2-(drift * time) + 3,3.9),
            np.random.normal(-0.5-(drift * time),7),
            np.random.normal(-1.2-(drift * time),8),
            np.random.normal(-3-(drift * time),7),
            np.random.normal(-10-(drift * time),20),
            x if x < (mean_7) + 3 else 50,
            np.random.normal(-1-(drift * time),2),
            np.random.normal(1-(drift * time),6),
            np.random.normal(0.7-(drift * time),4),
            np.random.normal(-6-(drift * time),11),
            np.random.normal(-7-(drift * time),1),
            np.random.normal(-0.5-(drift * time),2),
            np.random.normal(-6.5-(drift * time),1),
            np.random.normal(-3-(drift * time),6),
            np.random.normal(0-(drift * time),8),
            np.random.normal(2-(drift * time),3.9),
            np.random.normal(-9-(drift * time) + 3,12),
            np.random.normal(-1-(drift * time),6),
            np.random.normal(-4.5-(drift * time),8)              
        ]
    return probs

class explore_exploit():
    def __init__(self, epsilon:float = 0.01, sim_iter:int = 10000, num_machines:int = 21, convergance:int = 2, window_size:int = 10000,  drift:bool = False) -> None:
        self.__epsilon = epsilon
        self.__sim_iter = sim_iter
        self.__reward = 0
        self.__num_machines = num_machines
        self.__machines_info = []
        for i in range(self.__num_machines):
            self.__machines_info.append([0, 0]) #NOTE: the first index in the tuple is the number of times we have pull this machine, the second time step is the avg reward. 
        self.__best_machine_rn = 0
        self.__history = [] #this will keep track of each machines info.
        for i in range(self.__num_machines):
            self.__history.append([])
        self.__convergance = convergance
        self.__convergance_time_step = -1
        self.__history_of_avg = [0] * self.__sim_iter
        self.__window_size = window_size
        self.__last_window = [0] * self.__window_size
        self.__drift = drift
    def run_sim(self):
        convert_to_window_size = self.__sim_iter // self.__window_size
        for time in range(1, self.__sim_iter + 1):
            #save the old avg
            self.__history_of_avg[time - 1] = self.__reward
            sample = self.get_action(time)
            # to get the new avg of the last windo of samples (or how many we have if we have less than one window)
            if time < self.__window_size:
                self.__last_window[time] = sample
                tmp = sum(self.__last_window[:time])
                self.__reward = tmp / time
            else :
                self.__last_window[time % convert_to_window_size] = sample
                tmp = sum(self.__last_window)
                self.__reward = tmp / self.__window_size

        return (self.__reward, self.__history_of_avg) 
    def get_action(self, time=0):
        if self.__drift:
            draw = get_probabilities(time=time)
        else :
            draw = get_probabilities()
        if random.random() > self.__epsilon: #exploit
            self.__machines_info[self.__best_machine_rn][0] += 1 #incress the number of pulls by one. 
            # to get the new avg mulitiply by the num of pulls - 1, then add the current reward, then divde by the number of pulls
            self.__machines_info[self.__best_machine_rn][1] = ((self.__machines_info[self.__best_machine_rn][1] * (self.__machines_info[self.__best_machine_rn][0] - 1)) + draw[self.__best_machine_rn]) / self.__machines_info[self.__best_machine_rn][0]
            # Track History
            self.__history[self.__best_machine_rn].append(self.__machines_info[self.__best_machine_rn][1])
            return draw[self.__best_machine_rn]
        else : #explore 
            machine_to_explore = random.randint(0, self.__num_machines - 1)
            self.__machines_info[machine_to_explore][0] += 1 #incress the number of pulls by one. 
            # to get the new avg mulitiply by the num of pulls - 1, then add the current reward, then divde by the number of pulls
            self.__machines_info[machine_to_explore][1] = ((self.__machines_info[machine_to_explore][1] * (self.__machines_info[machine_to_explore][0] - 1)) + draw[machine_to_explore]) / self.__machines_info[machine_to_explore][0]
            #update the best machine 
            if self.__machines_info[machine_to_explore][1] > self.__machines_info[self.__best_machine_rn][1]:
                self.__best_machine_rn = machine_to_explore
            # Track History
            self.__history[machine_to_explore].append(self.__machines_info[machine_to_explore][1])
            return draw[machine_to_explore]
def main():
    explore_one = explore_exploit(epsilon=0.01, num_machines=num_machines, convergance=convergance, sim_iter=sim_iter, drift=drift)
    explore_two = explore_exploit(epsilon=0.05, num_machines=num_machines, convergance=convergance, sim_iter=sim_iter, drift=drift)
    explore_three = explore_exploit(epsilon=0.4, num_machines=num_machines, convergance=convergance, sim_iter=sim_iter, drift=drift)
    run_info_one = explore_one.run_sim()
    run_info_two = explore_two.run_sim()
    run_info_three = explore_three.run_sim()

    data_one_history_arg  =  run_info_one[1]
    data_two_history_arg  =  run_info_two[1]
    data_three_history_arg  =  run_info_three[1]

    fig, axs1 = plt.subplots(1, 3, figsize=(18,8))
    axs1[0].plot(data_one_history_arg, label = "Epsilon 0.01 AVG reward Convergance Line")
    axs1[0].set_xlabel('Time step')
    axs1[0].set_ylabel('AVG reward')
    axs1[0].legend()

    axs1[1].plot(data_two_history_arg, label = "Epsilon 0.05 AVG reward Convergance Line")
    axs1[1].set_xlabel('Time step')
    axs1[1].set_ylabel('AVG reward')
    axs1[1].legend()


    axs1[2].plot(data_three_history_arg, label = "Epsilon 0.01 AVG reward Convergance Line")
    axs1[2].set_xlabel('Time step')
    axs1[2].set_ylabel('AVG reward')
    axs1[2].legend()

    plt.show()


if __name__ == "__main__":
    main()
        